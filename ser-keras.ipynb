{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 256618,
     "sourceType": "datasetVersion",
     "datasetId": 107620
    },
    {
     "sourceId": 639622,
     "sourceType": "datasetVersion",
     "datasetId": 316368
    },
    {
     "sourceId": 653195,
     "sourceType": "datasetVersion",
     "datasetId": 325566
    },
    {
     "sourceId": 671851,
     "sourceType": "datasetVersion",
     "datasetId": 338555
    },
    {
     "sourceId": 2683750,
     "sourceType": "datasetVersion",
     "datasetId": 1633508
    },
    {
     "sourceId": 275083,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 235561,
     "modelId": 257246
    },
    {
     "sourceId": 307707,
     "sourceType": "modelInstanceVersion",
     "isSourceIdPinned": true,
     "modelInstanceId": 262135,
     "modelId": 283270
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speech Emotion Recognition\n",
    "\n",
    "1. Gathering data\n",
    "2. Quick EDA\n",
    "3. Preprocess\n",
    "4. Extract features\n",
    "5. Train LSTM models with different parameters\n",
    "6. Evaluate\n",
    "\n",
    "Datasets:\n",
    "* Crowd-sourced Emotional Multimodal Actors Dataset (Crema-D)\n",
    "* Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)\n",
    "* Surrey Audio-Visual Expressed Emotion (Savee)\n",
    "* Toronto Emotional Speech Set (Tessa)"
   ],
   "metadata": {
    "id": "FI50rmE6StN1"
   }
  },
  {
   "cell_type": "code",
   "source": "%pip install pydub kagglehub",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DY_4VozvStN4",
    "outputId": "674eb14d-90e7-439c-e5d6-3df18c5b3746"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nimport pandas as pd\nimport numpy as np\nimport kagglehub",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "id": "Uw8RBSaEStN4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# from tensorflow.keras.models import load_model\nimport tensorflow as tf\ntf.config.experimental.enable_op_determinism()\n# model = load_model(\"../input/lstm/keras/default/1/multi.h5\")",
   "metadata": {
    "trusted": true,
    "id": "hRZ8xHMyStN5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# RAVDESS = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n# CREMA = \"/kaggle/input/cremad/AudioWAV/\"\n# TESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n# SAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"",
   "metadata": {
    "trusted": true,
    "id": "w-nLkyRUStN5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**1. Ravdess Dataframe**\n\nThere are 1440 audio files, for example, 03-01-**06**-01-02-01-12.wav.",
   "metadata": {
    "id": "PnsE3fiMStN6"
   }
  },
  {
   "cell_type": "code",
   "source": "CREMA = kagglehub.dataset_download('ejlok1/cremad') + \"/AudioWAV/\"\nRAVDESS = kagglehub.dataset_download('uwrfkaggler/ravdess-emotional-speech-audio') + \"/audio_speech_actors_01-24/\"\nSAVEE = kagglehub.dataset_download('ejlok1/surrey-audiovisual-expressed-emotion-savee') + \"/ALL/\"\nTESS = kagglehub.dataset_download('ejlok1/toronto-emotional-speech-set-tess') + \"/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqPcSFV0Yy8Y",
    "outputId": "2006b022-fcc3-4d6b-ea01-f0a4c40212bf",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!ls /root/.cache/kagglehub/datasets/ejlok1/toronto-emotional-speech-set-tess/versions/1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gd_NalWhZqRy",
    "outputId": "e37254c3-ad2c-4c0c-eb52-63d266480022",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "ravdess_dir_lis = os.listdir(RAVDESS)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {\n    '03' : 'happy',\n    '01' : 'neutral',\n    '04' : 'sad',\n    '05' : 'angry',\n    '06' : 'fear',\n    '07' : 'disgust',\n}\n\nfor directory in ravdess_dir_lis:\n    actor_files = os.listdir(os.path.join(RAVDESS, directory))\n    for audio_file in actor_files:\n        part = audio_file.split('.')[0]\n        key = part.split('-')[2]\n        if key in emotion_dic:\n            gender_code = int(part.split('-')[6])\n            path_list.append(f\"{RAVDESS}{directory}/{audio_file}\")\n            gender_list.append('female' if gender_code & 1 == 0 else 'male')\n            emotion_list.append(emotion_dic[key])\n\nravdess_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nravdess_df.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "AF0ggjaiStN6",
    "outputId": "f4a3402a-2acf-477c-f8cc-f938f98f78d7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**2. Crema-D Dataframe**\n\nThere are 7,442 audio files, for example, 1001_DFA_**ANG**_XX.wav.",
   "metadata": {
    "id": "66SAXhy6StN7"
   }
  },
  {
   "cell_type": "code",
   "source": "crema_dir_list = os.listdir(CREMA)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {\n    'HAP' : 'happy',\n    'NEU' : 'neutral',\n    'SAD' : 'sad',\n    'ANG' : 'angry',\n    'FEA' : 'fear',\n    'DIS' : 'disgust',\n}\n\nfemale_id_list = [\n    '1002', '1003', '1004', '1006', '1007', '1008', '1009', '1010', '1012', '1013', '1018',\n    '1020', '1021', '1024', '1025', '1028', '1029', '1030', '1037', '1043', '1046', '1047',\n    '1049', '1052', '1053', '1054', '1055', '1056', '1058', '1060', '1061', '1063', '1072',\n    '1073', '1074', '1075', '1076', '1078', '1079', '1082', '1084', '1089', '1091',\n]\n\nfor audio_file in crema_dir_list:\n    part = audio_file.split('_')\n    key = part[2]\n    if key in emotion_dic and part[3] == 'HI.wav':\n        path_list.append(f\"{CREMA}{audio_file}\")\n        gender_list.append('female' if part[0] in female_id_list else 'male')\n        emotion_list.append(emotion_dic[key])\n\ncrema_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\ncrema_df.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K5WwqexnStN7",
    "outputId": "51b7fa34-ef86-4687-f0aa-7a3a0cd73ec3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**3. Tess Dataframe**\n\nThere are 2,800 audio files, for example, OAF_base_**fear**.wav.",
   "metadata": {
    "id": "F1aPtnLuStN7"
   }
  },
  {
   "cell_type": "code",
   "source": "tess_dir_list = os.listdir(TESS)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {\n    'happy'   : 'happy',\n    'neutral' : 'neutral',\n    'sad'     : 'sad',\n    'Sad'     : 'sad',\n    'angry'   : 'angry',\n    'fear'    : 'fear',\n    'disgust'  : 'disgust',\n}\n\nfor directory in tess_dir_list:\n    audio_files = os.listdir(os.path.join(TESS, directory))\n    for audio_file in audio_files:\n        part = audio_file.split('.')[0]\n        key = part.split('_')[2]\n        if key in emotion_dic:\n            path_list.append(f\"{TESS}{directory}/{audio_file}\")\n            gender_list.append('female') # female only dataset\n            emotion_list.append(emotion_dic[key])\n\ntess_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\ntess_df.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JpMmT8vkStN8",
    "outputId": "18f29ee8-aa8e-4e55-c82b-2f804250143d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "**4. Savee Dataframe**\n\nThere are 480 audio files, for example, DC_**a**02.wav.",
   "metadata": {
    "id": "qj1ne64mStN8"
   }
  },
  {
   "cell_type": "code",
   "source": "savee_dir_list = os.listdir(SAVEE)\npath_list = []\ngender_list = []\nemotion_list = []\n\nemotion_dic = {\n    'h'  : 'happy',\n    'n'  : 'neutral',\n    'sa' : 'sad',\n    'a'  : 'angry',\n    'f'  : 'fear',\n    'd'  : 'disgust'\n}\n\nfor audio_file in savee_dir_list:\n    part = audio_file.split('_')[1]\n    key = part[:-6]\n    if key in emotion_dic:\n        path_list.append(f\"{SAVEE}{audio_file}\")\n        gender_list.append('male') # male only dataset\n        emotion_list.append(emotion_dic[key])\n\nsavee_df = pd.concat([\n    pd.DataFrame(path_list, columns=['path']),\n    pd.DataFrame(gender_list, columns=['sex']),\n    pd.DataFrame(emotion_list, columns=['emotion'])\n], axis=1)\n\nsavee_df.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UCV2gGNEStN8",
    "outputId": "05cd4652-75d1-4373-e004-f514ccd90557"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df = pd.concat([\n    ravdess_df,\n    crema_df,\n    tess_df,\n    savee_df\n], axis=0)\ndf.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "C3WtPoWlStN8",
    "outputId": "6f716057-52db-4348-cd10-bf34076ee7e6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df.iloc[0][\"path\"]",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "C3WtPoWlStN8",
    "outputId": "6f716057-52db-4348-cd10-bf34076ee7e6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 2. Quick EDA\n\nWe check for imbalances like male to female ratio.",
   "metadata": {
    "id": "l8EFga0-StN9"
   }
  },
  {
   "cell_type": "code",
   "source": "import librosa\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')",
   "metadata": {
    "trusted": true,
    "id": "NV5dJEGMStN9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def plot_distribution(df):\n    countTable = df.groupby(['emotion', 'sex']).count()\n    pivotTable = countTable.pivot_table(index='emotion', columns='sex', values='path')\n\n    pivotTable.plot(kind='bar', figsize=(6, 3), color=['pink', 'blue'])\n    plt.title('Emotion and Gender Distribution')\n    plt.xlabel('Emotion')\n    plt.ylabel('Count')\n    plt.show()\n\nplot_distribution(df)",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "k17pZXCbStN9",
    "outputId": "fb54ae72-f0b2-4ad3-dba4-4a162fe4e7fa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# I decided to go with a female specific model\ndf = df[df['sex'] == 'female']\nplot_distribution(df)",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "4g-6L9DzStN9",
    "outputId": "87e1a3ce-6f51-4834-b8c7-e336a9385acd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import shutil\n\nshutil.copy(df.iloc[8][\"path\"], \"/kaggle/working/f5.wav\") ",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df.drop('sex', axis=1, inplace=True)\ndf.head()",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zDgCzZpvStN-",
    "outputId": "813da6d5-8149-45bf-cefb-a7328b78860b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df.iloc[0][\"path\"]",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from IPython.display import Audio\n\ndef create_waveplot(y, sr, title):\n    plt.figure(figsize=(8, 2))\n    plt.title(title)\n    librosa.display.waveshow(y, sr=sr)\n    plt.show()",
   "metadata": {
    "trusted": true,
    "id": "UGVx56qLStN-"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "emotion_label = 'sad'\ntitle = f\"Waveplot for {emotion_label} emotion\"\npath = np.array(df.path[df.emotion == emotion_label])[1]\ny, sr = librosa.load(path)\n\ncreate_waveplot(y, sr, title)\nAudio(path)",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "7CDlHyq9StN-",
    "outputId": "6c6231b4-fd30-46a1-cb2a-25d0ed2cc993"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 3. Preprocess\n\nThe following steps will be followed to preprocess the audio:\n\n1. Get an array of samples\n2. Trim the silence  \n3. Padding for equal length",
   "metadata": {
    "id": "kFULgtdAStN-"
   }
  },
  {
   "cell_type": "code",
   "source": "from pydub import AudioSegment, effects",
   "metadata": {
    "trusted": true,
    "id": "OzhRmg_SStN_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def preprocess_audio(path):\n    _, sr = librosa.load(path)\n    raw_audio = AudioSegment.from_file(path)\n\n    samples = np.array(raw_audio.get_array_of_samples(), dtype='float32')\n    trimmed, _ = librosa.effects.trim(samples, top_db=25)\n    padded = np.pad(trimmed, (0, 180000-len(trimmed)), 'constant')\n    return padded, sr",
   "metadata": {
    "trusted": true,
    "id": "-HpT4ch5StN_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "emotion_dic = {\n    'neutral' : 0,\n    'happy'   : 1,\n    'sad'     : 2,\n    'angry'   : 3,\n    'fear'    : 4,\n    'disgust' : 5\n}\n\ndef encode(label):\n    return emotion_dic.get(label)",
   "metadata": {
    "trusted": true,
    "id": "20UX4Oy7StN_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 4. Extract features\n\nWe will only extract these features:\n\n1. Mel-Frequency Cepstral Coefficients: captures the shape of the spectral envelope of a signal\n2. Zero Crossing Rate: captures the number of times a signal changes sign per second\n3. Root Mean Square Energy: captures the root mean square amplitude of the audio signal",
   "metadata": {
    "id": "kpVO31-cStN_"
   }
  },
  {
   "cell_type": "code",
   "source": "zcr_list = []\nrms_list = []\nmfccs_list = []\nemotion_list = []\n\nFRAME_LENGTH = 2048\nHOP_LENGTH = 512\n\nfor row in df.itertuples(index=False):\n    try:\n        y, sr = preprocess_audio(row.path)\n\n        zcr = librosa.feature.zero_crossing_rate(y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        rms = librosa.feature.rms(y=y, frame_length=FRAME_LENGTH, hop_length=HOP_LENGTH)\n        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=HOP_LENGTH)\n\n        zcr_list.append(zcr)\n        rms_list.append(rms)\n        mfccs_list.append(mfccs)\n\n        emotion_list.append(encode(row.emotion))\n    except:\n        print(f\"Failed for path: {row.path}\")",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZY2iCHaLStN_",
    "outputId": "6dcd339e-22a8-4abc-d7ab-42c32acee55f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X = np.concatenate((\n    np.swapaxes(zcr_list, 1, 2),\n    np.swapaxes(rms_list, 1, 2),\n    np.swapaxes(mfccs_list, 1, 2)),\n    axis=2\n)\nX = X.astype('float32')\n\ny = np.asarray(emotion_list)\ny = np.expand_dims(y, axis=1).astype('int8')",
   "metadata": {
    "trusted": true,
    "id": "lLQ4Uf0FStN_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 5. Build a LSTM\n\nBefore building the model, we will have to setup the data. LSTM are great for sequences.  ",
   "metadata": {
    "id": "bq6syBWaStN_"
   }
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical",
   "metadata": {
    "trusted": true,
    "id": "uuJwUCMdStN_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X_train, X_to_split, y_train, y_to_split = train_test_split(X, y, test_size=0.12, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_to_split, y_to_split, test_size=0.3, random_state=1)\n\ny_train_class = to_categorical(y_train, 6)\ny_val_class = to_categorical(y_val, 6)",
   "metadata": {
    "trusted": true,
    "id": "PYvMoerWStOA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from keras.models import Sequential\nfrom keras import layers, optimizers, callbacks, Model",
   "metadata": {
    "trusted": true,
    "id": "aGnb8i4CStOA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "X.shape",
   "metadata": {
    "trusted": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "haMuhsciStOA",
    "outputId": "dd116f80-6457-4e9d-87d2-5876d5a4dfd3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\ndef accuracy(y_true, y_pred_probs):\n    # print(y_true)\n    # print(y_pred_probs)\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    return np.sum(y_true == y_pred) / len(y_true)\n\ndef precision(y_true, y_pred_probs, average='macro'):\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    unique_classes = np.unique(y_true)\n    precisions = []\n\n    for cls in unique_classes:\n        tp = np.sum((y_true == cls) & (y_pred == cls))\n        fp = np.sum((y_true != cls) & (y_pred == cls))\n        p = tp / (tp + fp) if (tp + fp) > 0 else 0\n        precisions.append(p)\n\n    return np.mean(precisions) if average == 'macro' else np.sum(precisions * np.bincount(y_true) / len(y_true))\n\ndef recall(y_true, y_pred_probs, average='macro'):\n    y_pred = np.argmax(y_pred_probs, axis=1)\n    unique_classes = np.unique(y_true)\n    recalls = []\n\n    for cls in unique_classes:\n        tp = np.sum((y_true == cls) & (y_pred == cls))\n        fn = np.sum((y_true == cls) & (y_pred != cls))\n        r = tp / (tp + fn) if (tp + fn) > 0 else 0\n        recalls.append(r)\n\n    return np.mean(recalls) if average == 'macro' else np.sum(recalls * np.bincount(y_true) / len(y_true))\n\ndef f1_score(y_true, y_pred_probs, average='macro'):\n    p = precision(y_true, y_pred_probs, average=average)\n    r = recall(y_true, y_pred_probs, average=average)\n    return 2 * (p * r) / (p + r) if (p + r) > 0 else 0\n",
   "metadata": {
    "trusted": true,
    "id": "P71yhdmYStOB"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import keras\n\nclass BaseModel(Model):\n  def __init__(self, **kwargs):\n    super().__init__()\n    self.seq = Sequential()\n\n  def call(self, inputs):\n    return self.seq(inputs)\n\n  def build(self):\n    self.seq.build()\n\n@keras.saving.register_keras_serializable()\nclass LSTM1(BaseModel):\n  def __init__(self, activation):\n    super().__init__()\n    self.seq = Sequential([\n        layers.Input(shape=(352, 15)),\n        layers.LSTM(64, activation),\n        layers.Dense(6, activation=\"softmax\")\n    ])\n\n@keras.saving.register_keras_serializable()\nclass LSTM2(BaseModel):\n  def __init__(self, activation):\n    super().__init__()\n    self.seq = Sequential([\n        layers.Input(shape=(352, 15)),\n        layers.LSTM(84, activation, return_sequences=True),\n        layers.Dropout(0.2),\n        layers.LSTM(40, activation),\n        layers.Dropout(0.2),\n        layers.Dense(6, activation=\"softmax\")\n    ])\n\n@keras.saving.register_keras_serializable()\nclass LSTM3(BaseModel):\n  def __init__(self, activation, **kwargs):\n    super().__init__( **kwargs)\n    self.activation = activation\n    self.seq = Sequential([\n        layers.Input(shape=(352, 15)),\n        layers.Bidirectional(layers.LSTM(128, activation, return_sequences=True)),\n        layers.Dropout(0.3),\n        layers.Bidirectional(layers.LSTM(64, activation)),\n        layers.Dropout(0.4),\n        layers.Dense(6, activation=\"softmax\")\n    ])\n  def get_config(self):\n      return {\"activation\": self.activation}\n\n@keras.saving.register_keras_serializable()\nclass LSTM4(BaseModel):\n  def __init__(self, activation):\n    super().__init__()\n    self.seq = Sequential([\n        layers.Input(shape=(352, 15)),\n        layers.LSTM(128, activation, return_sequences=True),\n        layers.Dropout(0.2),\n        layers.LSTM(64, activation, return_sequences=True),\n        layers.Dropout(0.2),\n        layers.LSTM(64, activation),\n        layers.Dropout(0.2),\n        layers.Dense(6, activation=\"softmax\")\n    ])",
   "metadata": {
    "id": "sOrHg6XkUCqT",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model = LSTM1(\"relu\")\nmodel.compile()\n# model.predict(X)\n\nmodel = LSTM2(\"relu\")\nmodel.compile()\n# model.predict(X)\n\nmodel = LSTM3(\"relu\")\nmodel.compile()\n# model.predict(X)\n\nmodel = LSTM4(\"relu\")\nmodel.compile()\n# model.predict(X)\n",
   "metadata": {
    "id": "mALqARGedM8p",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def evaluate(model, X, y):\n  y_pred = model.predict(X)\n  y = y.flatten()\n  return {\n      \"accuracy\": accuracy(y, y_pred),\n      \"precision\": precision(y, y_pred),\n      \"recall\": recall(y, y_pred),\n      \"f1_score\": f1_score(y, y_pred)\n  }",
   "metadata": {
    "id": "1Cb-j1WXK5Ja",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import keras.optimizers as optim\nimport keras.utils\n\ndef train_all_models(X_train, y_train, X_val, y_val):\n  \"\"\"Train all architectures with some predefined hyperparameters\"\"\"\n  models = {model.__name__: model for model in (LSTM1, LSTM2, LSTM3, LSTM4)}\n\n  print(models)\n  histories = {}\n  for name, model in models.items():\n    print(f\"Model: {name}\")\n    keras.utils.set_random_seed(14)\n    model = model(\"sigmoid\")\n    model.compile(loss='categorical_crossentropy', optimizer=optim.RMSprop(learning_rate=0.001), metrics=['categorical_accuracy'])\n    history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val))\n    histories[name] = history.history\n    models[name] = model\n\n  return models, histories",
   "metadata": {
    "id": "kgQVBIFNViep",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def train_different_optimizers(model_class, optimizers, X_train, y_train, X_val, y_val):\n  \"\"\"Train model with different optimizers\"\"\"\n  histories = {}\n  models = {}\n  for name, optimizer in optimizers.items():\n    print(f\"Model: {model_class.__name__}, Optimizer: {name}\")\n    keras.utils.set_random_seed(14)\n    model = model_class(\"sigmoid\")\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer(learning_rate=0.001), metrics=['categorical_accuracy'])\n\n    history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val))\n    histories[name] = history.history\n    models[name] = model\n\n  return models, histories",
   "metadata": {
    "id": "TkJ06B6CbxXi",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def train_different_lrs(model_class, optimizer, lrs, X_train, y_train, X_val, y_val):\n  \"\"\"Train model with different learning rates\"\"\"\n  histories = {}\n  models = {}\n  for lr in lrs:\n    print(f\"Model: {model_class.__name__}, Optimizer: {optimizer.__name__}, LR: {lr}\")\n    keras.utils.set_random_seed(14)\n    model = model_class(\"sigmoid\")\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer(learning_rate=lr), metrics=['categorical_accuracy'])\n\n    history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val))\n    histories[str(lr)] = history.history\n    models[str(lr)] = model\n\n  return models, histories\n\ndef train_different_activations(model_class, optimizer, lr, activations, X_train, y_train, X_val, y_val):\n  \"\"\"Train model with different activation functions\"\"\"\n  histories = {}\n  models = {}\n  for activation in activations:\n    print(f\"Model: {model_class.__name__}, Optimizer: {optimizer.__name__}, LR: {lr}, Activation: {activation}\")\n    keras.utils.set_random_seed(14)\n    model = model_class(activation)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer(learning_rate=lr), metrics=['categorical_accuracy'])\n\n    history = model.fit(X_train, y_train, epochs=10, batch_size=8, validation_data=(X_val, y_val))\n    histories[activation] = history.history\n    models[activation] = model\n\n  return models, histories\n\ndef pipeline():\n  \"\"\"Best model selection through consecutive steps\"\"\"\n  models, histories = train_all_models(X_train, y_train_class, X_val, y_val_class)\n  evaluations = {name: evaluate(model, X_test, y_test) for name, model in models.items()}\n  best_name = max(evaluations, key=lambda x: evaluations[x][\"accuracy\"])\n  best_model = type(models[best_name])\n  print(f\"Best accuracy: {best_name}\")\n\n  optimizers = {\"RMSprop\": optim.RMSprop, \"Adam\": optim.Adam}\n  models, histories = train_different_optimizers(best_model, optimizers, X_train, y_train_class, X_val, y_val_class)\n  evaluations = {name: evaluate(model, X_test, y_test) for name, model in models.items()}\n  best_name = max(evaluations, key=lambda x: evaluations[x][\"accuracy\"])\n  best_optim = optimizers[best_name]\n  print(f\"Best accuracy: {best_name}\")\n\n  lrs = [0.001, 0.01, 0.1, 0.0001]\n  models, histories = train_different_lrs(best_model, best_optim, lrs, X_train, y_train_class, X_val, y_val_class)\n  evaluations = {name: evaluate(model, X_test, y_test) for name, model in models.items()}\n  best_name = max(evaluations, key=lambda x: evaluations[x][\"accuracy\"])\n  best_lr = float(best_name)\n  print(f\"Best accuracy: {best_name}\")\n\n  activations = [\"relu\", \"sigmoid\", \"tanh\"]\n  models, histories = train_different_activations(best_model, best_optim, best_lr, activations, X_train, y_train_class, X_val, y_val_class)\n  evaluations = {name: evaluate(model, X_test, y_test) for name, model in models.items()}\n  best_name = max(evaluations, key=lambda x: evaluations[x][\"accuracy\"])\n\n  print(f\"Best {best_name}\")\n    \n  return models[best_name], evaluations[best_name], histories[best_name]\n",
   "metadata": {
    "id": "3E_XTyAbg2mu",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "best_model, best_scores, best_history = pipeline()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ado1zTZfG77H",
    "outputId": "ebaa1bb8-4c13-45ae-fb4e-6fc54ef7cd27",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "y_train",
   "metadata": {
    "id": "V5ITN5AlfMmH",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "best_model.save(\"best.keras\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# import keras\n\nbest_model = keras.models.load_model(\"best.keras\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import json\n\nwith open(\"scores_.json\", \"w+\") as f:\n    f.write(json.dumps(best_scores))",
   "metadata": {
    "id": "V5ITN5AlfMmH",
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import json\n\nwith open(\"history_.json\", \"w+\") as f:\n    f.write(json.dumps(best_history))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "y_pred = best_model.predict(X_test)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "y_pred[:3]",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "np.argmax(y_pred, axis=1)[:3]",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "y_test[:3]",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "metrics = evaluate(best_model, X_test, y_test)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "metrics",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\nnp.argmax(y_pred, axis=1)\ny_test.flatten()",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def confusion_matrix(y_pred, y_true):\n    shape = np.max(y_true) + 1\n    y_pred = np.argmax(y_pred, axis=1)\n    # print(y_pred)\n    # print(y_true)\n    cm = np.zeros((shape, shape), dtype=np.int32)\n    for pred, true in zip(y_pred, y_true):\n        cm[true][pred] += 1\n    \n    return cm",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "cm = confusion_matrix(y_pred, y_test.flatten())",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "cm",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\nemotion_dic = {\n    'neutral' : 0,\n    'happy'   : 1,\n    'sad'     : 2,\n    'angry'   : 3,\n    'fear'    : 4,\n    'disgust' : 5\n}\nplt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = False, labeltop=True)\nsns.heatmap(cm, annot=True, cmap=sns.color_palette(\"viridis\", as_cmap=True), xticklabels=emotion_dic.keys(), yticklabels=emotion_dic.keys())\nplt.savefig(\"heatmap.png\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "list(emotion_dic.keys())",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "best_history",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "best_model.layers[1].layers[0].backward_layer.activation",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\naxes[0].plot(best_history['loss'])\naxes[0].plot(best_history['val_loss'])\naxes[0].set_title('Loss for Train and Validation Sets')\naxes[0].set_ylabel('Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].legend(['Training', 'Validation'])\n\naxes[1].plot(best_history['categorical_accuracy'])\naxes[1].plot(best_history['val_categorical_accuracy'])\naxes[1].set_title('Accuracy for Train and Validation Sets')\naxes[1].set_ylabel('Accuracy')\naxes[1].set_xlabel('Epochs')\naxes[1].legend(['Training', 'Validation'])\n\nfig.tight_layout()\n\nplt.savefig(\"training.png\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# 6. Evaluate and conclude\n\nLet's see how good are model is.",
   "metadata": {
    "id": "AAMKscQ3StOB"
   }
  }
 ]
}
